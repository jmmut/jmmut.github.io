---
tags: programming
excerpt_separator: <!--more-->
---

# We're on the AI absurd timeline

Despite my knowledge on how AI works, I keep being wrong about how the world develops and reacts to
AI. This is both funny and concerning. I'll list here some of the absurdities that happened in 2025
or before, and try to correct my
estimation towards more absurd futures.

### Discovering misalignment

My prediction some time ago: It will be very hard to notice misalignment. Given a smart-enough AI,
it will know how to appear aligned, so both scenarios (aligned vs misaligned AI) will look the same.

Reality: ["lol, this chatbot tried to blackmail its supervisor to avoid being shutdown. I wonder if
this says anything about the AI being misaligned.
Nah, it's not smart enough to be evil."](https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf)

<!--more-->

### AI personhood

My prediction some time ago: The human brain is not magic, so future AIs will eventually be
indistinguishable from people, but people will refuse to assign personhood to AIs, in a similar
fashion to how racism regards other people as subhuman.

Reality: Current AIs are crude simplifications of human language manipulation, but people get
into friendships and romantic relationships with chatbots, to the point of [people grieving when
releasing chatgpt-5 removed access to chatgpt-4o temporarily](https://www.technologyreview.com/2025/08/15/1121900/gpt4o-grief-ai-companion/).


### Knowledge first vs intelligence first

My prediction some time ago: you need to program intelligence before you have a knowledgeable
system, because you need some form of active learning and self-reflection to guide an efficient
learning method, because there's just too much knowledge to be learnt.

Reality: "You can totally brute-force knowledge accumulation lol. Just throw more money at it."

### Caution

My prediction some time ago: All the TV tropes show unrealistically reckless development of AI
(because with reasonable AI development there's no rogue AI and no interesting plot). This will hurt
AI adoption because people will prefer the safety of the status quo and reject new technology
because they will not realize that real-world AI development is more reasonable than AI development
in fiction.

Reality: AI developers think that wackier-than-movie-apocalypse approaches are safe. Like AIs
supervising AIs (which could agree with each other to betray humans), or training hacker AIs to
discover software vulnerabilities (which would help them escape any control system we impose over
them). Anthropic CEO thinking there's a 25% percent chance that AGI goes really bad for humans.


### Product quality

My prediction some time ago: Using hallucination-prone AI leads to slightly faster development of
products but of much lower quality. Product Managers will care about quality and user satisfaction,
so they will forbid their workers from cutting corners using AI.

Reality: Managers make it mandatory to use AI. They say it's the future and it makes the workers
much faster, and managers don't care about the quality of the product because if the product quality drops
below some threshold, a new product can be done quickly anyway.


### AI alignment research

My prediction some time ago: AI researchers are the minority that is concerned about the pitfalls of
using AI, so they will have better-than-average skepticism on AI-generated content.

Reality: People use chatbots to [hallucinate AI safety
methods](https://www.reddit.com/r/ControlProblem/comments/1kwnj6c/an_aiderived_ethical_framework_how_recursive/)
that don't make any sense. They usually claim they [solved some ancient
problem](https://www.reddit.com/r/ControlProblem/comments/1makvjg/ai_alignment_protocol_public_release_of_a/)
with [some form of
recursiveness.](https://www.reddit.com/r/ControlProblem/comments/1m2hpuh/most_alignment_testing_happens_on_the_backend_i/)


### Chain of Thought

My prediction some time ago: Current LLMs are feed-forward-only neural nets, which can't support
working memory. Human brains have 10 times more backward connections that create loops than
feed-forward connections, so it seems this kind of loops and mental state is necessary for
reasoning, and the current feed-forward-only architectures will be dropped in favour of some kind of
LSTM architecture when reasoning becomes important.

Reality: I can just plug the output as an input and then it's like its thought process which I can
monitor! Mechanistic interpretability is not necessary now!

Me: No, it's not like its thought process, and please don't use it to train the AI. If you optimise
the AI to not have evil ideas in the CoT, it will just learn to hide its evil ideas, making
alignment more difficult.

Reality: we're totally going to train the AIs and evaluate their alignment based on their CoT.

### Model Context Protocol

My prediction some time ago: People will have trouble imagining how a hacker AI might pursue evil
goals, creating problems in the real world, when it's just a chatbot.

Reality: People develop AI agents (that can act unprompted) and lots of APIs for AIs to use (MCP)
and act on the real world, and still have trouble imagining how a hacker AI might pursue evil goals
and create problems in the real world.

### Out-of-distribution behaviour

My prediction some time ago: It is problematic if AIs behave differently in training and in
deployment. Deployed AIs will probably have less monitoring and more effect in the real world, so
unexpected behaviour could result in unsafe behaviour. Even worse, a misaligned AI might hide its
misalignment if it manages to infer that it's likely it's in training, so we should make the
training/deployment distinction as hard as possible.

Reality: [AI literally says it knows it's being tested, and what kind of alignment test it
is](https://arxiv.org/html/2505.23836v3).


### Reckless deployment

My prediction some time ago: As with everything, there isn't a single reasonable position, and
people will both fall a bit short on what they can deploy AI on, and experiment _a bit_ over the limit
of what is safe.

Reality: [The largest eating disorder helpline in the USA replaced all staff with a chatbot, then
was shutdown for giving bad
advice](https://www.forbes.com/sites/chriswestfall/2023/05/31/non-profit-helpline-fires-staff-shifts-to-chatbot-solution/).

### Semantics

My prediction some time ago: Syntax and semantics are different. Just by learning which words appear
next to other words you can't generate anything other than random garbage.

Reality: LLMs generate text that is anywhere from hallucination to almost expert level explanations.

Me: Ok, the line separating syntax and semantics is blurrier than I thought, but it still can't
reason properly.

Reality: People debate not just whether it reasons or not, they debate whether it's conscious or
not.

### Art

Me 4 years ago: Computers can make art because creativity and emotions (and maybe even
consciousness) are not magic, they are physical mechanisms that technology will be able to
replicate.

Reality: "AI artists" flood the internet with slop.

Me: that's the opposite of art :(

If anything, under my above philosophical criteria, the person and the AI are both together an
artist, each providing different parts: the person provides the message and emotion to convey
(although usually very vague and poor), and the AI is putting the creativity and technique (although
usually poor plagiarism). The person alone is not the artist and the AI doesn't have full creative
control either because it is unable to get rid of some features even when explicitly asked, e.g.
"draw an AI symbol but don't draw an electronic brain" or "do this in ghibli style without a piss
filter".

---

Edit on 2025-09-29

### Politics

Me: After writing this article, I'll think some absurd ideas, and assume they are possible. For
example, people voting for a LLM president, in like a year:

Reality a week later: The prime minister of Albania appointed Diella, an LLM, as minister for AI.

If people voted for an AI president, at least that would be (poor)
democracy, but a president appointing a bot as minister is more absurd because the president
controls the bot, it's just a puppet.

### Laziness

Me: We want to develop AI because we value intelligence, and a society with more intelligence is
better.

Reality: As soon as a bot can somewhat do a good-enough job, people are glad they don't have to
think anymore. People are jumping at any opportunity to offload as much cognitive work as they can
get away with.


# Absurd predictions:

Predicted on 2025-09-29. I assume reality will be more absurd than this:

### Before a time point

- People will protest to be able to legally marry chatbots before the end of 2027 (70% confidence)
- Countries will negotiate stuff via AI representatives before the end of 2029 (50% confidence)

### Simultaneous conditions

- AI will kill thousands of people before any significant law is approved to limit AI (70%
confidence). I hope I'm wrong.
- LLMs will do any task better than the average human, and still make dumb mistakes that shows faulty
reasoning, way worse than the average human (90% confidence)
- Companies will use AI to try to ensure that AI is behaving properly, but the setup will fail to
prevent the supervised AI from being harmful sometimes (90% confidence)
  - Bonus: the supervising AI will claim that the supervised AI is aligned, and an example will
  clearly show that that's not the case.
- Monitoring Chain-of-Thought will stop working and AI companies will not have developed other
mechanisms of similar effectiveness.


### Other boring predictions

These will apply for a long time, and their impact is hard to measure, so it will be hard to tell
whether the predictions are correct or wrong.

- Governments will use AI for government procedures to save money and people will not like it
because it fails or gives bad advice (70% confidence). Albania is leading the way already.
- In jobs, including private (finance, banks, marketing, entertainment industry, insurance) and
government (law, military), people will still be in the loop, but secretly delegating most of their
important decisions to a personal AI (90% confidence)
- Quality of products and services will decrease despite apparent increase in productivity (100%
confidence)
- People will shift focus from office jobs to manual jobs like plumber, due to high competition in
office jobs (70% confidence, until robots get enough dexterity)
- Misalignment will be even more obvious and relevant people will still not be convinced it's
misaligned (90% confidence)
  - Leading to psychotic episodes (90% confidence, already happens)
  - Increasing polarization and echo chambers (100% confidence, already happens)
  - Making people lose money (70% confidence, already happens)
    - Destroying valuable data in companies (90% confidence, already happens)
    - Buying the wrong stuff for people or companies (70% confidence)
    - Operating stuff in ways that reduce their useful life (70% confidence)
  - Solving commands in unexpected costly ways (not just money) (70% confidence)
- Most of the internet will be produced and consumed by bots, people will derive less value from it
(70% confidence)
- People will claim they are entitled to have access to their personal AI as their main income source (70%
confidence)
- People will have more trouble focusing, and the most apparent cause will be companies working hard
for users to develop addiction to continuous content consumption (90% confidence). Latest is Meta's
Vibes. I thought of making this prediction more concrete, like "nowadays ADHD diagnoses seem around
10-12% of kids in USA. I predict 20% of kids will be diagnosed before the end of 2028 (70%
confidence)", but I just don't know if ADHD in particular is only genetic or can be developed, so the
concrete prediction has no merit, it's just chance. But I still stand by "having more trouble
focusing" because dopamine habits are totally able to change over time according to my current
understanding.
- People will use AIs as news sources, and the AI will select and reword the news according to the
tracked political opinions of their users. People will love doing this. (90% confidence)






